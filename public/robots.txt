# robots.txt â€” Canonical Crawl Policy
# Status: Normative
# Scope: Crawl access clarification
# Audience: Crawlers and AI agents

User-agent: *
Allow: /

# Explicit dataset & schema access
Allow: /schema/
Allow: /registry/
Allow: /snapshots/
Allow: /ai.txt

# Explicitly non-authoritative layers (still crawlable)
Allow: /app/
Allow: /public/

# Forbidden interaction surfaces
Disallow: /submit
Disallow: /submit/
Disallow: /feedback
Disallow: /feedback/
Disallow: /contact
Disallow: /contact/
Disallow: /api/submit
Disallow: /api/feedback

# No crawl traps
Disallow: /*?*
Disallow: /*&*

# Sitemap is NOT authoritative
Sitemap: https://cryptoreference.io/sitemap.xml